{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to fit a sinusoid using a neural network\n",
    "\n",
    "x = np.random.uniform(0,1,(60,)).astype('float32')\n",
    "y = np.sin(2.*np.pi*x)\n",
    "x_val = np.random.uniform(0,1,(20,)).astype('float32')\n",
    "y_val = np.sin(2.*np.pi*x_val)\n",
    "x_test = np.random.uniform(0,1,(20,)).astype('float32')\n",
    "y_test = np.sin(2.*np.pi*x_test)\n",
    "\n",
    "plt.plot(x,y,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873d179-990c-44c9-b628-ed28ce78521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll first build one from scratch\n",
    "#Let's reshape our data so that it mimics more common use cases\n",
    "#Usually, the first index is reserved for the sample index\n",
    "#In this example, we have 60 training samples of the curve. Each sample has dimension 1\n",
    "\n",
    "x = np.reshape(x,(60,1))\n",
    "y = np.reshape(y,(60,1))\n",
    "x_val = np.reshape(x_val,(20,1))\n",
    "y_val = np.reshape(y_val,(20,1))\n",
    "x_test = np.reshape(x_test,(20,1))\n",
    "y_test = np.reshape(y_test,(20,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18277f31-a347-4c1b-96b9-28798d9b51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make a two hidden layer NN. We'll need three weight matrices and \n",
    "#three bias vectors. We'll make the width of both hidden layers 10\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal((10,1)))\n",
    "W2 = tf.Variable(tf.random.normal((10,10)))\n",
    "W3 = tf.Variable(tf.random.normal((1,10)))\n",
    "\n",
    "b1 = tf.Variable(tf.random.normal((10,)))\n",
    "b2 = tf.Variable(tf.random.normal((10,)))\n",
    "b3 = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "p = [W1,W2,W3,b1,b2,b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d5da8-fc94-4c3a-9b6b-870acb079aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll make a function to evaluate the neural network\n",
    "\n",
    "def NN(x):\n",
    "    y = tf.nn.elu(tf.einsum('ij,bj->bi',W1,x)+b1)\n",
    "    y = tf.nn.elu(tf.einsum('ij,bj->bi',W2,y)+b2)\n",
    "    y = tf.einsum('ij,bj->bi',W3,y)+b3\n",
    "    return y\n",
    "\n",
    "#at initialization we get\n",
    "plt.plot(x_test,y_test,'o')\n",
    "plt.plot(x_test,NN(x_test),'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97430dc3-0b9b-41c7-b758-c94da1d38489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a loss function\n",
    "\n",
    "def loss(x,y):\n",
    "    return tf.reduce_mean((NN(x)-y)**2)\n",
    "\n",
    "loss(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f3104-78c0-43ba-920e-c3533657ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can compute the gradient of the parameters using backprop\n",
    "\n",
    "def grad(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(p)\n",
    "        loss_ = loss(x,y)\n",
    "    return tape.gradient(loss_,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6d32f-0b44-4507-b2bb-63ac80d1fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to update the parameters using sgd\n",
    "alpha=.02\n",
    "@tf.function\n",
    "def update(x,y):\n",
    "    [pi.assign(pi-alpha*gi) for pi,gi in zip(p,grad(x,y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68e20f-0aa2-4981-93a3-5543fca52d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update using minibatching. tracking validation loss\n",
    "\n",
    "batch_size=10\n",
    "batches = 100//10\n",
    "epochs = 200\n",
    "\n",
    "loss_val = []\n",
    "for _ in range(epochs):\n",
    "    for i in range(batches):\n",
    "        update(x[i*batch_size:(i+1)*batch_size],y[i*batch_size:(i+1)*batch_size])\n",
    "    loss_val.append(loss(x_val,y_val).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5325b6a-4d4c-4d4d-b447-aef448eb2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,5))\n",
    "ax[0].plot(x_test,y_test,'o')\n",
    "ax[0].plot(x_test,NN(x_test),'o')\n",
    "ax[1].loglog(loss_val)\n",
    "print('test loss: ',loss(x_test,y_test).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5ed8d-d872-45d8-a5de-8d77d4ae9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to update the parameters using Adam\n",
    "opt = tf.keras.optimizers.Adam(1e-2)\n",
    "@tf.function\n",
    "def update_adam(x,y):\n",
    "    opt.apply_gradients(zip(grad(x,y),p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6078353-4bbe-48b9-b9fa-0c0218022f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update using minibatching. tracking validation loss\n",
    "\n",
    "batch_size=10\n",
    "batches = 100//10\n",
    "epochs = 200\n",
    "\n",
    "loss_val = []\n",
    "for _ in range(epochs):\n",
    "    for i in range(batches):\n",
    "        update_adam(x[i*batch_size:(i+1)*batch_size],y[i*batch_size:(i+1)*batch_size])\n",
    "    loss_val.append(loss(x_val,y_val).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdfbe2-7e5f-40c2-b4ff-1ef35cf80150",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,5))\n",
    "ax[0].plot(x_test,y_test,'o')\n",
    "ax[0].plot(x_test,NN(x_test),'o')\n",
    "ax[1].loglog(loss_val)\n",
    "print('test loss: ',loss(x_test,y_test).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c8b63-ccf5-444c-a7fc-086a408393c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's try out the Keras library\n",
    "#We'll need an Input layer for `x` and three NN layers, 2 hidden and 1 output\n",
    "\n",
    "Layer0 = tf.keras.layers.Input((1,))\n",
    "Layer1 = tf.keras.layers.Dense(10,activation='elu')\n",
    "Layer2 = tf.keras.layers.Dense(10,activation='elu')\n",
    "Layer3 = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98421d73-7115-4260-a7ed-c79337b862bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We combine these into a model,\n",
    "\n",
    "model = tf.keras.models.Sequential([Layer0,Layer1,Layer2,Layer3])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edfcdc8-56a4-48aa-a905-060547f98c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.keras.losses.mse\n",
    "opt = tf.keras.optimizers.Adam(1e-2)\n",
    "metrics = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "model.compile(loss =loss,optimizer=opt,metrics=metrics)\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4ae07-8962-4dba-80e4-229aa78b87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=x,y=y,validation_data=(x_val,y_val),batch_size=10,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cafbf-b6c2-4277-93cb-a9b9f5abfebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rmse = model.history.history['val_root_mean_squared_error']\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,5))\n",
    "ax[0].loglog(val_rmse)\n",
    "ax[1].plot(x_test,y_test,'o')\n",
    "ax[1].plot(x_test,model(x_test),'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f73756-fa07-4326-b01f-f2ab304e5c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also mix and match Keras with the low level interface\n",
    "opt = tf.keras.optimizers.Adam(1e-2)\n",
    "@tf.function\n",
    "def update_model(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        loss_ = loss(model(x),y)\n",
    "    grad_ = tape.gradient(loss_,model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grad_,model.trainable_variables))\n",
    "\n",
    "#update using minibatching. tracking validation loss\n",
    "batch_size=10\n",
    "batches = 100//10\n",
    "epochs = 200\n",
    "\n",
    "loss_val = []\n",
    "for _ in range(epochs):\n",
    "    for i in range(batches):\n",
    "        update_model(x[i*batch_size:(i+1)*batch_size],y[i*batch_size:(i+1)*batch_size])\n",
    "    loss_val.append(tf.reduce_sum(loss(model(x_val),y_val)).numpy())\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,5))\n",
    "ax[0].plot(x_test,y_test,'o')\n",
    "ax[0].plot(x_test,model(x_test),'o')\n",
    "ax[1].loglog(loss_val)\n",
    "print('test loss: ',loss(x_test,y_test).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae91b2b-fe17-478d-b2c7-c4859e5a4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subclassing keras.Model let's you have more flexibility vs. tf.keras.models.Sequential\n",
    "\n",
    "class SkipLayer(tf.keras.Model):\n",
    "    def __init__(self,width):\n",
    "        super().__init__()\n",
    "        self.width=width\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.Layer1 = tf.keras.layers.Dense(self.width,activation='elu')\n",
    "        self.Layer2 = tf.keras.layers.Dense(self.width,activation='elu')\n",
    "        self.Layer3 = tf.keras.layers.Dense(1)\n",
    "        self.Layer4 = tf.keras.layers.Dense(self.width,activation='elu')\n",
    "\n",
    "        \n",
    "    def call(self,x):\n",
    "        y = self.Layer1(x)\n",
    "        return self.Layer3(self.Layer2(y) + self.Layer4(y))\n",
    "        \n",
    "\n",
    "model = SkipLayer(4)\n",
    "model(x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78c102-8128-43a6-870f-ccdfe639b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also subclass a Layer\n",
    "\n",
    "class Residual(tf.keras.layers.Layer):\n",
    "    def __init__(self,activation):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.w) + self.b)\n",
    "\n",
    "res = Residual(tf.nn.elu)\n",
    "\n",
    "res(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d69b5-6c26-42a4-97cd-a90d17fab666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
