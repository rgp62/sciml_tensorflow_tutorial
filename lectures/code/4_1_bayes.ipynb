{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow_probability as tfp\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a439a-8423-4fe6-adb8-6ba49308ef44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d27142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data and true function\n",
    "x_all = np.linspace(0,1,100)\n",
    "y_true = x_all**2+.5*x_all-.5\n",
    "\n",
    "x_data = np.reshape((np.array([0.25,.75])[None]*np.ones((5,))[:,None]),(-1,))\n",
    "# x_data = np.random.uniform(.1,.5,50)\n",
    "y_data = (x_data**2+.5*x_data-.5) + np.random.normal(0,.05,len(x_data))\n",
    "\n",
    "def plot_data():\n",
    "    plt.plot(x_all,y_true,label='True function')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.plot(x_data,y_data,'ro',label='Data points')\n",
    "    \n",
    "plot_data()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Design matrix for data and test points\n",
    "def makeX(x):\n",
    "    return np.stack([np.ones(len(x)),x,x**2],axis=-1)\n",
    "X = makeX(x_data)\n",
    "print(X)\n",
    "X_all = makeX(x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can't invert the normal matrix\n",
    "a = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One option is to use the pseudoinverse\n",
    "a_pinv = np.linalg.pinv(X.T.dot(X)).dot(X.T.dot(y_data))\n",
    "y_pinv = X_all.dot(a_pinv)\n",
    "\n",
    "plot_data()\n",
    "plt.plot(x_all,y_pinv,label='Pseudoinverse')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or we can regularize\n",
    "a_reg1 = np.linalg.inv(X.T.dot(X)+1e-1*np.eye(3)).dot(X.T.dot(y_data))\n",
    "a_reg2 = np.linalg.inv(X.T.dot(X)+1e-4*np.eye(3)).dot(X.T.dot(y_data))\n",
    "\n",
    "y_reg1 = X_all.dot(a_reg1)\n",
    "y_reg2 = X_all.dot(a_reg2)\n",
    "\n",
    "plot_data()\n",
    "plt.plot(x_all,y_reg1,label=r'Regularize: $\\alpha = 10^{-1}$')\n",
    "plt.plot(x_all,y_reg2,label=r'Regularize: $\\alpha = 10^{-4}$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf21932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian inference\n",
    "sig_l = 1e0\n",
    "sig_p = 1e0\n",
    "mu = np.linalg.solve(X.T.dot(X) + sig_l**2/sig_p**2 * np.eye(3),X.T.dot(y_data))\n",
    "Omega = np.linalg.inv(1./sig_l**2*X.T.dot(X) + 1./sig_p**2 * np.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1180f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prior predictive plot\n",
    "nu = X_all.dot(np.zeros_like(mu))\n",
    "sig_y = X_all.dot(np.eye(3)*sig_p**2).dot(X_all.T)\n",
    "sig_y_diag = np.sqrt(np.diag(sig_y))\n",
    "plot_data()\n",
    "plt.plot(x_all,nu,label = 'Prior predictive mean')\n",
    "plt.fill_between(x_all,nu-sig_y_diag,nu+sig_y_diag,color='C4',alpha=.3,label=r'95% of confidence')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posterior predictive plot\n",
    "nu = X_all.dot(mu)\n",
    "sig_y = X_all.dot(Omega).dot(X_all.T)\n",
    "sig_y_diag = np.sqrt(np.diag(sig_y))\n",
    "plot_data()\n",
    "plt.plot(x_all,nu,label = 'Posterior predictive mean')\n",
    "plt.fill_between(x_all,nu-sig_y_diag,nu+sig_y_diag,color='C4',alpha=.3,label=r'95% of confidence')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde25755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type II likelihood optimization\n",
    "\n",
    "losses=[]\n",
    "sigs = tf.Variable([1e0,1e0],dtype=tf.float64)\n",
    "XT=X.T\n",
    "X2 = X.T.dot(X)\n",
    "X2_ = X.dot(X.T)\n",
    "Id = tf.eye(len(y_data),dtype=tf.float64)\n",
    "I3 = tf.eye(3,dtype=tf.float64)\n",
    "XTy = X.T.dot(y_data)\n",
    "\n",
    "def negLogLikelihood(sigs_p):\n",
    "    sigs = tf.nn.relu(sigs_p)+1e-5\n",
    "\n",
    "    Omega = tf.linalg.inv(1./sigs[0]**2*X2 + 1./sigs[1]**2 * I3)\n",
    "    mu = tf.einsum('ij,jk,k->i',Omega,XT,y_data)/sigs[0]**2\n",
    "    nu = tf.einsum('ij,j->i',X,mu)\n",
    "    \n",
    "    sig_y = Id*sigs[0]**2+tf.einsum('ij,jk,kl',X,Omega,XT)\n",
    "    sig_y_m = tf.reduce_sum((y_data-nu)*tf.linalg.solve(sig_y,(y_data-nu)[:,None])[:,0])\n",
    "\n",
    "    return tf.linalg.logdet(sig_y)+sig_y_m\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def grad(sigs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(sigs)\n",
    "        loss_ = negLogLikelihood(sigs)\n",
    "    grad = tape.gradient(loss_,sigs)\n",
    "    return grad\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(1e-3)\n",
    "@tf.function\n",
    "def GD():\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(sigs)\n",
    "        loss_ = negLogLikelihood(sigs)\n",
    "    grad = tape.gradient(loss_,sigs)\n",
    "    opt.apply_gradients([(grad,sigs)])\n",
    "    return loss_\n",
    "for _ in range(1000): \n",
    "    losses.append(GD().numpy())\n",
    "\n",
    "\n",
    "        \n",
    "plt.plot(np.array(losses))\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Negative log likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d57dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian inference\n",
    "sig_l = sigs[0]\n",
    "sig_p = sigs[1]\n",
    "\n",
    "#Posterior predictive plot\n",
    "Omega = np.linalg.inv(1./sig_l**2*X.T.dot(X) + 1./sig_p**2 * np.eye(3))\n",
    "sqrtOmega = np.linalg.cholesky(Omega)\n",
    "mu = Omega.dot(X.T.dot(y_data))/sig_l**2\n",
    "nu = X_all.dot(mu)\n",
    "sig_y = np.eye(len(x_all))*sig_l**2+X_all.dot(Omega).dot(X_all.T)\n",
    "sig_y_diag = np.sqrt(np.diag(sig_y))\n",
    "plot_data()\n",
    "plt.plot(x_all,nu,label = 'Posterior predictive mean')\n",
    "plt.fill_between(x_all,nu-2.*sig_y_diag,nu+2.*sig_y_diag,color='C4',alpha=.3,label=r'95% of confidence')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can observe two types of uncertainty, aleatoric and epistemic if we add noise to the data\n",
    "#Aleatoric: inherent stochasticity in the data\n",
    "#Epistemic: uncertainty due to lack of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c2b55-0145-4918-84db-29effbca8252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
